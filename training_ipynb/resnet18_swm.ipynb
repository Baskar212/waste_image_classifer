{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "JCA3in7RwY6Y",
        "outputId": "ac73bd5d-d4a3-498f-aaa1-ce8f2b55934c"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Repository cloned successfully!\n"
          ]
        }
      ],
      "source": [
        "import os\n",
        "import git\n",
        "\n",
        "REPO_URL = \"https://github.com/nikhilvenkatkumsetty/TrashBox.git\"\n",
        "DATASET_DIR = \"TrashBox\"\n",
        "\n",
        "if not os.path.exists(DATASET_DIR):\n",
        "    git.Repo.clone_from(REPO_URL, DATASET_DIR)\n",
        "    print(\"Repository cloned successfully!\")\n",
        "else:\n",
        "    print(\"Repository already exists.\")\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def list_dataset_contents(dataset_dir):\n",
        "    \"\"\"List dataset structure with categories and subcategories.\"\"\"\n",
        "    dataset_structure = {}\n",
        "    for category in sorted(os.listdir(dataset_dir)):\n",
        "        category_path = os.path.join(dataset_dir, category)\n",
        "        if os.path.isdir(category_path):\n",
        "            dataset_structure[category] = sorted(os.listdir(category_path))\n",
        "    return dataset_structure\n",
        "\n",
        "dataset_structure = list_dataset_contents(DATASET_DIR)\n",
        "print(\"Dataset Structure:\", dataset_structure)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-NcXvj6V0_9V",
        "outputId": "533c93d4-96f5-4e06-9b73-cad9fbace2af"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Dataset Structure: {'.git': ['HEAD', 'branches', 'config', 'description', 'hooks', 'index', 'info', 'logs', 'objects', 'packed-refs', 'refs'], 'TrashBox_train_dataset_subfolders': ['cardboard', 'e-waste', 'glass', 'medical', 'metal', 'paper', 'plastic'], 'TrashBox_train_set': ['cardboard', 'e-waste', 'glass', 'medical', 'metal', 'paper', 'plastic']}\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import git\n",
        "import torch\n",
        "import torchvision\n",
        "import torchvision.transforms as transforms\n",
        "from torchvision import datasets, models\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "from torch.utils.data import DataLoader\n",
        "from PIL import Image\n",
        "\n",
        "# Step 1: Clone dataset if not already present\n",
        "REPO_URL = \"https://github.com/nikhilvenkatkumsetty/TrashBox.git\"\n",
        "DATASET_PATH = \"TrashBox/TrashBox_train_set\"\n",
        "\n",
        "if not os.path.exists(\"TrashBox\"):\n",
        "    print(\"Cloning dataset...\")\n",
        "    git.Repo.clone_from(REPO_URL, \"TrashBox\")\n",
        "    print(\"Dataset cloned successfully!\")\n",
        "\n",
        "# Step 2: Check for valid images\n",
        "def is_valid_image(file_path):\n",
        "    \"\"\"Check if a file is a valid image.\"\"\"\n",
        "    try:\n",
        "        img = Image.open(file_path)\n",
        "        img.verify()\n",
        "        return True\n",
        "    except:\n",
        "        return False\n",
        "\n",
        "# Convert images to standard format\n",
        "for root, _, files in os.walk(DATASET_PATH):\n",
        "    for file in files:\n",
        "        file_path = os.path.join(root, file)\n",
        "        if is_valid_image(file_path):\n",
        "            try:\n",
        "                img = Image.open(file_path).convert(\"RGB\")\n",
        "                img.save(file_path, \"JPEG\")  # Save as JPEG\n",
        "            except:\n",
        "                print(f\"Skipping corrupt file: {file_path}\")\n",
        "        else:\n",
        "            print(f\"Removing non-image file: {file_path}\")\n",
        "            os.remove(file_path)\n",
        "\n",
        "# Step 3: Define transformations\n",
        "transform = transforms.Compose([\n",
        "    transforms.Resize((224, 224)),\n",
        "    transforms.ToTensor(),\n",
        "    transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n",
        "])\n",
        "\n",
        "# Step 4: Load dataset\n",
        "dataset = datasets.ImageFolder(root=DATASET_PATH, transform=transform)\n",
        "dataloader = DataLoader(dataset, batch_size=32, shuffle=True)\n",
        "class_names = dataset.classes\n",
        "print(\"Classes:\", class_names)\n",
        "\n",
        "# Step 5: Load pre-trained ResNet-18 model\n",
        "model = models.resnet18(pretrained=True)\n",
        "num_features = model.fc.in_features\n",
        "model.fc = nn.Linear(num_features, len(class_names))\n",
        "\n",
        "# Define loss function and optimizer\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "optimizer = optim.Adam(model.parameters(), lr=0.001)\n",
        "\n",
        "# Move to GPU if available\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "model = model.to(device)\n",
        "\n",
        "# Step 6: Training loop\n",
        "num_epochs = 5\n",
        "for epoch in range(num_epochs):\n",
        "    model.train()\n",
        "    running_loss = 0.0\n",
        "\n",
        "    for images, labels in dataloader:\n",
        "        images, labels = images.to(device), labels.to(device)\n",
        "\n",
        "        optimizer.zero_grad()\n",
        "        outputs = model(images)\n",
        "        loss = criterion(outputs, labels)\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "        running_loss += loss.item()\n",
        "\n",
        "    print(f\"Epoch [{epoch+1}/{num_epochs}], Loss: {running_loss/len(dataloader):.4f}\")\n",
        "\n",
        "# Step 7: Save trained model\n",
        "torch.save(model.state_dict(), \"waste_classifier.pth\")\n",
        "print(\"Model training complete and saved!\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "VsrMQdPIxcqh",
        "outputId": "9b5f44e9-3e73-487c-8676-e7557d809856"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Removing non-image file: TrashBox/TrashBox_train_set/paper/paper 2273.jpg\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/PIL/Image.py:1045: UserWarning: Palette images with Transparency expressed in bytes should be converted to RGBA images\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Removing non-image file: TrashBox/TrashBox_train_set/e-waste/e-waste 1719.jpg\n",
            "Removing non-image file: TrashBox/TrashBox_train_set/cardboard/cardboard 1075.jpg\n",
            "Classes: ['cardboard', 'e-waste', 'glass', 'medical', 'metal', 'paper', 'plastic']\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/torchvision/models/_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and may be removed in the future, please use 'weights' instead.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.11/dist-packages/torchvision/models/_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=ResNet18_Weights.IMAGENET1K_V1`. You can also use `weights=ResNet18_Weights.DEFAULT` to get the most up-to-date weights.\n",
            "  warnings.warn(msg)\n",
            "Downloading: \"https://download.pytorch.org/models/resnet18-f37072fd.pth\" to /root/.cache/torch/hub/checkpoints/resnet18-f37072fd.pth\n",
            "100%|██████████| 44.7M/44.7M [00:00<00:00, 168MB/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch [1/5], Loss: 1.1440\n",
            "Epoch [2/5], Loss: 0.8070\n",
            "Epoch [3/5], Loss: 0.6502\n",
            "Epoch [4/5], Loss: 0.5276\n",
            "Epoch [5/5], Loss: 0.4240\n",
            "Model training complete and saved!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import shutil\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "# Define the directory paths\n",
        "train_dir = 'TrashBox/TrashBox_train_set'  # Path to your training data\n",
        "test_dir = 'TrashBox/TrashBox_test_set'   # Path to your test data\n",
        "\n",
        "# Create test directory if it doesn't exist\n",
        "if not os.path.exists(test_dir):\n",
        "    os.makedirs(test_dir)\n",
        "\n",
        "# List classes (folders) in the training directory\n",
        "classes = os.listdir(train_dir)\n",
        "\n",
        "# Split each class folder into train and test sets\n",
        "for class_name in classes:\n",
        "    class_path = os.path.join(train_dir, class_name)\n",
        "\n",
        "    # List all image files in the class folder\n",
        "    images = os.listdir(class_path)\n",
        "\n",
        "    # Split the images into train and test (80% train, 20% test)\n",
        "    train_images, test_images = train_test_split(images, test_size=0.2, random_state=42)\n",
        "\n",
        "    # Create test folder for the class if it doesn't exist\n",
        "    test_class_path = os.path.join(test_dir, class_name)\n",
        "    if not os.path.exists(test_class_path):\n",
        "        os.makedirs(test_class_path)\n",
        "\n",
        "    # Move images to the test folder\n",
        "    for img in test_images:\n",
        "        shutil.move(os.path.join(class_path, img), os.path.join(test_class_path, img))\n",
        "\n",
        "print(\"Dataset split completed! Test set created at:\", test_dir)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "lc2XJO3s4g1z",
        "outputId": "7b07b0bf-c28b-40f8-dac8-e9fa286a64a3"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Dataset split completed! Test set created at: TrashBox/TrashBox_test_set\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Path to your test dataset\n",
        "test_dir = 'TrashBox/TrashBox_test_set'\n",
        "\n",
        "# Define the transformations for the test dataset\n",
        "test_transform = transforms.Compose([\n",
        "    transforms.Resize((224, 224)),  # Resize to match model input size\n",
        "    transforms.ToTensor(),\n",
        "    transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n",
        "])\n",
        "\n",
        "# Load the test dataset\n",
        "test_dataset = datasets.ImageFolder(root=test_dir, transform=test_transform)\n",
        "\n",
        "# Create the DataLoader for the test dataset\n",
        "test_loader = DataLoader(test_dataset, batch_size=32, shuffle=False, num_workers=4)\n",
        "\n",
        "# Evaluate the model\n",
        "model.eval()  # Set the model to evaluation mode\n",
        "correct = 0\n",
        "total = 0\n",
        "\n",
        "with torch.no_grad():  # No need to compute gradients for evaluation\n",
        "    for images, labels in test_loader:\n",
        "        images, labels = images.to(device), labels.to(device)\n",
        "        outputs = model(images)\n",
        "        _, predicted = torch.max(outputs, 1)\n",
        "        total += labels.size(0)\n",
        "        correct += (predicted == labels).sum().item()\n",
        "\n",
        "accuracy = 100 * correct / total\n",
        "print(f\"Accuracy of the model on the test images: {accuracy:.2f}%\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Z1WAliYo4zvq",
        "outputId": "d9fb8346-fc42-4dd3-c43c-8909b3b76b2b"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/torch/utils/data/dataloader.py:617: UserWarning: This DataLoader will create 4 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Accuracy of the model on the test images: 87.93%\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from torch.utils.data import DataLoader\n",
        "from torchvision import datasets, transforms\n",
        "\n",
        "# Define transformations for the training dataset\n",
        "train_transform = transforms.Compose([\n",
        "    transforms.Resize((224, 224)),  # Resize images to 224x224 (required by most models)\n",
        "    transforms.ToTensor(),          # Convert images to PyTorch tensors\n",
        "    transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])  # Normalize with ImageNet values\n",
        "])\n",
        "\n",
        "# Define the path to the training dataset\n",
        "train_dir = 'TrashBox/TrashBox_train_set'\n",
        "\n",
        "# Load the training dataset\n",
        "train_dataset = datasets.ImageFolder(root=train_dir, transform=train_transform)\n",
        "\n",
        "# Create the DataLoader for the training dataset\n",
        "train_loader = DataLoader(train_dataset, batch_size=32, shuffle=True, num_workers=4)\n",
        "\n",
        "# Now, you can continue with the training loop\n"
      ],
      "metadata": {
        "id": "gIns6Yb95Us0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Continue training for more epochs (let's say, 5 more epochs)\n",
        "num_epochs = 5  # Set the number of additional epochs\n",
        "for epoch in range(6, 6 + num_epochs):  # Start from epoch 6 if you have already completed 5 epochs\n",
        "    model.train()\n",
        "    running_loss = 0.0\n",
        "\n",
        "    for images, labels in train_loader:\n",
        "        images, labels = images.to(device), labels.to(device)\n",
        "\n",
        "        # Zero the parameter gradients\n",
        "        optimizer.zero_grad()\n",
        "\n",
        "        # Forward pass\n",
        "        outputs = model(images)\n",
        "        loss = criterion(outputs, labels)\n",
        "\n",
        "        # Backward pass and optimize\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "        # Track running loss\n",
        "        running_loss += loss.item()\n",
        "\n",
        "    # Print statistics for every epoch\n",
        "    print(f\"Epoch [{epoch}/{6 + num_epochs}], Loss: {running_loss / len(train_loader):.4f}\")\n",
        "\n",
        "# Save the updated model\n",
        "torch.save(model.state_dict(), 'trashbox_model_updated.pth')\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "02n0t9c95kVj",
        "outputId": "03a233c5-be94-4c29-cd85-b3281dcca514"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch [6/11], Loss: 0.3324\n",
            "Epoch [7/11], Loss: 0.2247\n",
            "Epoch [8/11], Loss: 0.1945\n",
            "Epoch [9/11], Loss: 0.1653\n",
            "Epoch [10/11], Loss: 0.1635\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n",
        "# Define the path to save the model on Google Drive\n",
        "model_save_path = '/content/drive/MyDrive/trashbox_model.pth'\n",
        "\n",
        "# Save the model's state_dict to Google Drive\n",
        "torch.save(model.state_dict(), model_save_path)\n",
        "\n",
        "print(f\"Model saved to: {model_save_path}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7mAWT-tv8Q5B",
        "outputId": "35f84862-703a-431e-dbf5-6a42432d05a9"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n",
            "Model saved to: /content/drive/MyDrive/trashbox_model.pth\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torchvision.models as models\n",
        "from google.colab import drive, files\n",
        "from torchvision import transforms\n",
        "from PIL import Image\n",
        "import io\n",
        "\n",
        "# Mount Google Drive\n",
        "drive.mount('/content/drive')\n",
        "\n",
        "# Load the pre-trained ResNet model\n",
        "model = models.resnet18(pretrained=True)\n",
        "\n",
        "# Modify the final fully connected layer for your number of classes (7 categories in this case)\n",
        "model.fc = nn.Linear(model.fc.in_features, 7)  # Adjust this for your number of categories\n",
        "\n",
        "# Load the trained model weights (make sure the path is correct)\n",
        "model.load_state_dict(torch.load('/content/drive/MyDrive/trashbox_model.pth'))  # Adjust the path\n",
        "model.eval()\n",
        "\n",
        "# Define image transformations\n",
        "transform = transforms.Compose([\n",
        "    transforms.Resize((224, 224)),\n",
        "    transforms.ToTensor(),\n",
        "    transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n",
        "])\n",
        "\n",
        "# Categories (replace with your actual categories)\n",
        "categories = [\"Cardboard\", \"E-Waste\", \"Glass\", \"Medical\", \"Metal\", \"Paper\", \"Plastic\"]\n",
        "\n",
        "# Function to classify uploaded image\n",
        "def classify_uploaded_image():\n",
        "    uploaded = files.upload()  # Upload an image file\n",
        "\n",
        "    for filename in uploaded.keys():\n",
        "        # Open the uploaded image\n",
        "        img = Image.open(io.BytesIO(uploaded[filename]))\n",
        "\n",
        "        # Apply the transformations\n",
        "        img_tensor = transform(img).unsqueeze(0)\n",
        "\n",
        "        # Classify the image\n",
        "        with torch.no_grad():\n",
        "            outputs = model(img_tensor)\n",
        "            _, predicted = torch.max(outputs, 1)\n",
        "            predicted_class = categories[predicted.item()]\n",
        "\n",
        "        # Show result\n",
        "        print(f\"Predicted Category: {predicted_class}\")\n",
        "        img.show()  # Display the uploaded image\n",
        "\n",
        "# Step 5: Classify the uploaded image\n",
        "classify_uploaded_image()\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 267
        },
        "id": "mEnih17zC0b4",
        "outputId": "918d81de-c96a-43b7-a632-a0e64f958758"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/torchvision/models/_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and may be removed in the future, please use 'weights' instead.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.11/dist-packages/torchvision/models/_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=ResNet18_Weights.IMAGENET1K_V1`. You can also use `weights=ResNet18_Weights.DEFAULT` to get the most up-to-date weights.\n",
            "  warnings.warn(msg)\n",
            "Downloading: \"https://download.pytorch.org/models/resnet18-f37072fd.pth\" to /root/.cache/torch/hub/checkpoints/resnet18-f37072fd.pth\n",
            "100%|██████████| 44.7M/44.7M [00:00<00:00, 77.8MB/s]\n",
            "<ipython-input-4-e8ed51465f22>:19: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
            "  model.load_state_dict(torch.load('/content/drive/MyDrive/trashbox_model.pth'))  # Adjust the path\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "\n",
              "     <input type=\"file\" id=\"files-a1a5273c-2880-4d1f-8708-f135d08c56b8\" name=\"files[]\" multiple disabled\n",
              "        style=\"border:none\" />\n",
              "     <output id=\"result-a1a5273c-2880-4d1f-8708-f135d08c56b8\">\n",
              "      Upload widget is only available when the cell has been executed in the\n",
              "      current browser session. Please rerun this cell to enable.\n",
              "      </output>\n",
              "      <script>// Copyright 2017 Google LLC\n",
              "//\n",
              "// Licensed under the Apache License, Version 2.0 (the \"License\");\n",
              "// you may not use this file except in compliance with the License.\n",
              "// You may obtain a copy of the License at\n",
              "//\n",
              "//      http://www.apache.org/licenses/LICENSE-2.0\n",
              "//\n",
              "// Unless required by applicable law or agreed to in writing, software\n",
              "// distributed under the License is distributed on an \"AS IS\" BASIS,\n",
              "// WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n",
              "// See the License for the specific language governing permissions and\n",
              "// limitations under the License.\n",
              "\n",
              "/**\n",
              " * @fileoverview Helpers for google.colab Python module.\n",
              " */\n",
              "(function(scope) {\n",
              "function span(text, styleAttributes = {}) {\n",
              "  const element = document.createElement('span');\n",
              "  element.textContent = text;\n",
              "  for (const key of Object.keys(styleAttributes)) {\n",
              "    element.style[key] = styleAttributes[key];\n",
              "  }\n",
              "  return element;\n",
              "}\n",
              "\n",
              "// Max number of bytes which will be uploaded at a time.\n",
              "const MAX_PAYLOAD_SIZE = 100 * 1024;\n",
              "\n",
              "function _uploadFiles(inputId, outputId) {\n",
              "  const steps = uploadFilesStep(inputId, outputId);\n",
              "  const outputElement = document.getElementById(outputId);\n",
              "  // Cache steps on the outputElement to make it available for the next call\n",
              "  // to uploadFilesContinue from Python.\n",
              "  outputElement.steps = steps;\n",
              "\n",
              "  return _uploadFilesContinue(outputId);\n",
              "}\n",
              "\n",
              "// This is roughly an async generator (not supported in the browser yet),\n",
              "// where there are multiple asynchronous steps and the Python side is going\n",
              "// to poll for completion of each step.\n",
              "// This uses a Promise to block the python side on completion of each step,\n",
              "// then passes the result of the previous step as the input to the next step.\n",
              "function _uploadFilesContinue(outputId) {\n",
              "  const outputElement = document.getElementById(outputId);\n",
              "  const steps = outputElement.steps;\n",
              "\n",
              "  const next = steps.next(outputElement.lastPromiseValue);\n",
              "  return Promise.resolve(next.value.promise).then((value) => {\n",
              "    // Cache the last promise value to make it available to the next\n",
              "    // step of the generator.\n",
              "    outputElement.lastPromiseValue = value;\n",
              "    return next.value.response;\n",
              "  });\n",
              "}\n",
              "\n",
              "/**\n",
              " * Generator function which is called between each async step of the upload\n",
              " * process.\n",
              " * @param {string} inputId Element ID of the input file picker element.\n",
              " * @param {string} outputId Element ID of the output display.\n",
              " * @return {!Iterable<!Object>} Iterable of next steps.\n",
              " */\n",
              "function* uploadFilesStep(inputId, outputId) {\n",
              "  const inputElement = document.getElementById(inputId);\n",
              "  inputElement.disabled = false;\n",
              "\n",
              "  const outputElement = document.getElementById(outputId);\n",
              "  outputElement.innerHTML = '';\n",
              "\n",
              "  const pickedPromise = new Promise((resolve) => {\n",
              "    inputElement.addEventListener('change', (e) => {\n",
              "      resolve(e.target.files);\n",
              "    });\n",
              "  });\n",
              "\n",
              "  const cancel = document.createElement('button');\n",
              "  inputElement.parentElement.appendChild(cancel);\n",
              "  cancel.textContent = 'Cancel upload';\n",
              "  const cancelPromise = new Promise((resolve) => {\n",
              "    cancel.onclick = () => {\n",
              "      resolve(null);\n",
              "    };\n",
              "  });\n",
              "\n",
              "  // Wait for the user to pick the files.\n",
              "  const files = yield {\n",
              "    promise: Promise.race([pickedPromise, cancelPromise]),\n",
              "    response: {\n",
              "      action: 'starting',\n",
              "    }\n",
              "  };\n",
              "\n",
              "  cancel.remove();\n",
              "\n",
              "  // Disable the input element since further picks are not allowed.\n",
              "  inputElement.disabled = true;\n",
              "\n",
              "  if (!files) {\n",
              "    return {\n",
              "      response: {\n",
              "        action: 'complete',\n",
              "      }\n",
              "    };\n",
              "  }\n",
              "\n",
              "  for (const file of files) {\n",
              "    const li = document.createElement('li');\n",
              "    li.append(span(file.name, {fontWeight: 'bold'}));\n",
              "    li.append(span(\n",
              "        `(${file.type || 'n/a'}) - ${file.size} bytes, ` +\n",
              "        `last modified: ${\n",
              "            file.lastModifiedDate ? file.lastModifiedDate.toLocaleDateString() :\n",
              "                                    'n/a'} - `));\n",
              "    const percent = span('0% done');\n",
              "    li.appendChild(percent);\n",
              "\n",
              "    outputElement.appendChild(li);\n",
              "\n",
              "    const fileDataPromise = new Promise((resolve) => {\n",
              "      const reader = new FileReader();\n",
              "      reader.onload = (e) => {\n",
              "        resolve(e.target.result);\n",
              "      };\n",
              "      reader.readAsArrayBuffer(file);\n",
              "    });\n",
              "    // Wait for the data to be ready.\n",
              "    let fileData = yield {\n",
              "      promise: fileDataPromise,\n",
              "      response: {\n",
              "        action: 'continue',\n",
              "      }\n",
              "    };\n",
              "\n",
              "    // Use a chunked sending to avoid message size limits. See b/62115660.\n",
              "    let position = 0;\n",
              "    do {\n",
              "      const length = Math.min(fileData.byteLength - position, MAX_PAYLOAD_SIZE);\n",
              "      const chunk = new Uint8Array(fileData, position, length);\n",
              "      position += length;\n",
              "\n",
              "      const base64 = btoa(String.fromCharCode.apply(null, chunk));\n",
              "      yield {\n",
              "        response: {\n",
              "          action: 'append',\n",
              "          file: file.name,\n",
              "          data: base64,\n",
              "        },\n",
              "      };\n",
              "\n",
              "      let percentDone = fileData.byteLength === 0 ?\n",
              "          100 :\n",
              "          Math.round((position / fileData.byteLength) * 100);\n",
              "      percent.textContent = `${percentDone}% done`;\n",
              "\n",
              "    } while (position < fileData.byteLength);\n",
              "  }\n",
              "\n",
              "  // All done.\n",
              "  yield {\n",
              "    response: {\n",
              "      action: 'complete',\n",
              "    }\n",
              "  };\n",
              "}\n",
              "\n",
              "scope.google = scope.google || {};\n",
              "scope.google.colab = scope.google.colab || {};\n",
              "scope.google.colab._files = {\n",
              "  _uploadFiles,\n",
              "  _uploadFilesContinue,\n",
              "};\n",
              "})(self);\n",
              "</script> "
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Saving 360_F_498362712_7sJRmv7sOsfCtqieE0wtIjUpdUBvF4PY.jpg to 360_F_498362712_7sJRmv7sOsfCtqieE0wtIjUpdUBvF4PY.jpg\n",
            "Predicted Category: Plastic\n"
          ]
        }
      ]
    },
    {
      "source": [
        "from google.colab import files\n",
        "files.download('/content/drive/MyDrive/trashbox_model.pth')"
      ],
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 280
        },
        "id": "lN7oQOuN9ekc",
        "outputId": "47794117-6322-4092-9e79-7bb8326760e0",
        "collapsed": true
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "error",
          "ename": "FileNotFoundError",
          "evalue": "Cannot find file: /content/drive/MyDrive/trashbox_model.pth",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-4-b4a417f14fd3>\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mgoogle\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcolab\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mfiles\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mfiles\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdownload\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'/content/drive/MyDrive/trashbox_model.pth'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/google/colab/files.py\u001b[0m in \u001b[0;36mdownload\u001b[0;34m(filename)\u001b[0m\n\u001b[1;32m    231\u001b[0m   \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0m_os\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexists\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilename\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    232\u001b[0m     \u001b[0mmsg\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m'Cannot find file: {}'\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilename\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 233\u001b[0;31m     \u001b[0;32mraise\u001b[0m \u001b[0mFileNotFoundError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmsg\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# pylint: disable=undefined-variable\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    234\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    235\u001b[0m   \u001b[0mcomm_manager\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_IPython\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_ipython\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mkernel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcomm_manager\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mFileNotFoundError\u001b[0m: Cannot find file: /content/drive/MyDrive/trashbox_model.pth"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "C-y4A-kjh2sv"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}